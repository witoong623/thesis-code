{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'C:\\\\Users\\\\Hasegawa\\\\Desktop\\\\AIT Note\\\\Thesis\\\\other implementation\\\\Carla-ppo\\\\vae\\\\models\\\\seg_bce_cnn_zdim64_beta1_kl_tolerance0.0_data\\\\checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        latent_size = 64\n",
    "        image_size = (80, 160)\n",
    "        settings = [\n",
    "            # out channel, k, s, p\n",
    "            (16, 4, 2, 0),\n",
    "            (32, 4, 2, 0),\n",
    "            (32, 4, 2, 0),\n",
    "            (64, 4, 2, 0),\n",
    "            (64, 4, 2, 0),\n",
    "        ]\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        (self.encoded_H, self.encoded_W), size_hist = self._calculate_spatial_size(image_size, self.encoder)\n",
    "        \n",
    "        self.mean = nn.Linear(self.encoded_H * self.encoded_W * 256, latent_size)\n",
    "        self.logstd = nn.Linear(self.encoded_H * self.encoded_W * 256, latent_size)\n",
    "        \n",
    "        # latent\n",
    "        self.latent = nn.Linear(latent_size, self.encoded_H * self.encoded_W * 256)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        return self.mean(x), self.logstd(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.latent(z)\n",
    "        z = z.view(-1, 256, self.encoded_H, self.encoded_W)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, x, encode=False, mean=False):\n",
    "        mu, logstd = self.encode(x)\n",
    "        z = mu\n",
    "        x = self.decode(z)\n",
    "        return x, mu, logstd\n",
    "    \n",
    "    def _calculate_spatial_size(self, image_size, conv_layers):\n",
    "        ''' Calculate spatial size after convolution layers '''\n",
    "        H, W = image_size\n",
    "        size_hist = []\n",
    "        size_hist.append((H, W))\n",
    "\n",
    "        for layer in conv_layers:\n",
    "            if layer.__class__.__name__ != 'Conv2d':\n",
    "                continue\n",
    "            conv = layer\n",
    "            H = int((H + 2 * conv.padding[0] - conv.dilation[0] * (conv.kernel_size[0] - 1) - 1) / conv.stride[0] + 1)\n",
    "            W = int((W + 2 * conv.padding[1] - conv.dilation[1] * (conv.kernel_size[1] - 1) - 1) / conv.stride[1] + 1)\n",
    "\n",
    "            size_hist.append((H, W))\n",
    "\n",
    "        return (H, W), size_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.rand(1, 3, 80, 160)\n",
    "dummy_hat, mu, logstd = model(dummy)\n",
    "\n",
    "assert dummy_hat.size() == (1, 1, 80, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_name = [n for n, param in model.named_parameters()]\n",
    "params = [param for n, param in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encoder.0.weight',\n",
       " 'encoder.0.bias',\n",
       " 'encoder.2.weight',\n",
       " 'encoder.2.bias',\n",
       " 'encoder.4.weight',\n",
       " 'encoder.4.bias',\n",
       " 'encoder.6.weight',\n",
       " 'encoder.6.bias',\n",
       " 'mean.weight',\n",
       " 'mean.bias',\n",
       " 'logstd.weight',\n",
       " 'logstd.bias',\n",
       " 'latent.weight',\n",
       " 'latent.bias',\n",
       " 'decoder.0.weight',\n",
       " 'decoder.0.bias',\n",
       " 'decoder.2.weight',\n",
       " 'decoder.2.bias',\n",
       " 'decoder.4.weight',\n",
       " 'decoder.4.bias',\n",
       " 'decoder.6.weight',\n",
       " 'decoder.6.bias']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_vars = tf.train.list_variables(checkpoint_path)\n",
    "names = []\n",
    "arrays = []\n",
    "for name, shape in init_vars:\n",
    "    array = tf.train.load_variable(checkpoint_path, name)\n",
    "    names.append(name)\n",
    "    arrays.append(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = model.state_dict()\n",
    "new_state_dict = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch_conv_index(idx):\n",
    "    return str((idx - 1) * 2)\n",
    "\n",
    "def to_pretained_conv_index(idx):\n",
    "    return str(idx / 2 + 1)\n",
    "\n",
    "for name, array in zip(names, arrays):\n",
    "    splited_names = name.split('/')\n",
    "    # skip unnecessary parameter\n",
    "    if any(n in ['Adam', 'Adam_1', 'step_idx', 'beta1_power', 'beta2_power'] for n in splited_names):\n",
    "        continue\n",
    "\n",
    "    splited_names.pop(0)\n",
    "    # print(name)\n",
    "    \n",
    "    if splited_names[0] == 'encoder':\n",
    "        # splited_names[1] is in \"convX\" format where X is number\n",
    "        splited_names[1] = to_torch_conv_index(int(splited_names[1][-1]))\n",
    "    elif splited_names[0] == 'decoder':\n",
    "        if splited_names[1] == 'dense1':\n",
    "            splited_names = ['latent', splited_names[-1]]\n",
    "        else:\n",
    "            # splited_names[1] is in \"convX\" format where X is number\n",
    "            splited_names[1] = to_torch_conv_index(int(splited_names[1][-1]))\n",
    "    elif splited_names[0] == 'mean':\n",
    "        pass\n",
    "    elif splited_names[0] == 'logstd_sqare':\n",
    "        splited_names[0] = 'logstd'\n",
    "    else:\n",
    "        raise Exception(f'not support key: {name}')\n",
    "        \n",
    "    if splited_names[-1] == 'kernel':\n",
    "        splited_names[-1] = 'weight'\n",
    "        array = array.transpose()\n",
    "        \n",
    "    new_key = '.'.join(splited_names)\n",
    "        \n",
    "    current_param = model_state_dict[new_key]\n",
    "    if current_param.size() != array.shape:\n",
    "        raise Exception(f'key {new_key} has mismatch weight shape: {current_param.size()} and {array.shape}')\n",
    "        \n",
    "    new_state_dict[new_key] = torch.from_numpy(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_state_dict, 'carla-ppo-seg-vae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0, 2, 4, 6]) / 2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, 6])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([1, 2, 3, 4]) - 1) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
